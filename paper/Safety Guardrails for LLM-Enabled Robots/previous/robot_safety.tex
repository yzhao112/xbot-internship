Methods for robot safety verification have typically focused on ensuring robots satisfy precise safety specifications in known environments.  
Formal methods application, such as linear temporal logic (LTL) and planning domain definition language (PDDL), provide such a specification approach, and have been used for a range of robotic tasks~\cite{pnueli1977temporal, fox2003pddl2}. 
Tasks specified by formal methods enjoy guarantees on correctness and realizability~\cite{kress2018synthesis, wongpiromsarn2011tulip, vasile2013sampling}, enabling safety verification in uncertain, dynamic, and semantic environments~\cite{vasile2013sampling, shah2020planning, purohit2021dynamic_ltl, menghi2018multiuncertainty, fu2016optimalsemanticltl, kantaros2022perception}.
Formal methods also facilitate control synthesis given possibly conflicting specifications~\cite{tuumova2013minimumviolationltl}, which is particularly relevant when proposed robot plans conflict with safety specifications. 
However, these approaches often require a system designer to provide fixed specifications for a given, non-adversarial, environmental context.
%which limits their utility in open-world settings. 

A more recent line of work has sought to adapt techniques from the formal methods literature to meet the needs of LLM-enabled robots~\cite{liu2023grounding, quartey2024verifiably, chen2024autotamp, liu2023llmp}.  Such approaches typically restrict the LLM's planning syntax to a more narrowly defined formal language, enabling verification of LLM-generated, long-horizon plans to ensure feasibility and prevent hallucination~\cite{mavrogiannis2024cook2ltl, quartey2024verifiably}.  This approach has also enabled planning under conflicting specifications~\cite{optimalscenegraphllm}.  However, in the context of robotic safety, existing methods at the intersection of formal methods and LLM face two key challenges.

Firstly, existing methods typically require manual enumeration of safety specifications, preventing use in open-world settings.  This is particularly restrictive for approaches that require specifications to be provided in the language of formal methods~\cite{yang2024joint}.  \citet{yang2023plugsafetychipenforcing} propose a safety filter that relaxes these requirements by allowing for linguistic constraints, but their method still requires a designer to enumerate contextual constraints. 
%(\textit{e.g.}, ``only enter the kitchen after turning off the lights'').  
This line of work has been furthered by~\citet{brunke2024semantically}, who use LLMs to generate contextual constraints (\textit{e.g.,} ``don't move a cup of water over a laptop''), but facilitate neither the ability to edit constraints online nor the ability to reason about these constraints. And finally, while \citet{fadhil2024saycomplyllm} use LLM-based planners alongside external knowledge bases, they consider the setting of planning rather than safety. In contrast, \textsc{RoboGuard} \emph{automatically} reasons for contextual safety specifications given high-level safety rules from a system designer.

Secondly, existing work on LLM-enabled robot safety does not consider adversarial use cases~\cite{liu2024lang2ltl, chen2023nl2tl, chen2024autotamp}.  Although methods like LIMP~\cite{quartey2024verifiably} verifiably follow user instructions, they lack mechanisms to prevent an adversarial user from generating unsafe robot actions.  We address these limitations by showing that our guardrail is adversarially robust, even in cases wherein an adversary has full knowledge of our guardrail's internal state.
