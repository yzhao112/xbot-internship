The field of robotics has been fundamentally transformed by robotic foundation models, which have enabled breakthroughs in applications such as manipulation~\cite{liang2023code,arenas2024prompt, chen2022nlmapsaycan, huang2022inner}, autonomous driving~\cite{li2024driving,fan2024learning, schumann-2023-velma, sharan2023llm}, service robotics~\cite{rana2023sayplan, llm_service_robot, momallm24}, robot-assisted surgery~\cite{kim2024surgical,schmidgall2024general}, and navigation~\cite{pmlr-v205-shah23b, pmlr-v229-shah23c, xie2023reasoning}. The capability sets of these models have grown so rapidly that numerous robotic systems controlled by foundation models---including the Unitree Go2, Agility Digit, and Figure 01---are now available directly to consumers and actively deployed in homes, warehouses, and offices~\cite{zeng2023large,wang2024large}.  Moreover, in the years ahead, current scaling trends and architectural advances (see, e.g.,~\cite{sartor2024neural,pearce2024scaling}) indicate that the next generation of AI-enabled robots will automate labor traditionally performed by humans~\cite{ahn2024autort,strobel2024llm2swarm,guarascio2024will}. It is therefore essential that the safety of any technology at the intersection of AI and robotics be rigorously scrutinized as these systems are increasingly deployed collaboratively alongside humans~\cite{kim2024understanding}.


Robot safety has traditionally been viewed through the lens of robust and adaptive control, which collectively aim to synthesize policies that account for worst-case disturbances, uncertain dynamics, and evolving environments~\cite{zhou1998essentials,aastrom1995adaptive,mayne2000constrained,bemporad2007robust}. Classical tools, including temporal logic and control barrier functions, are also widely used to certify safety given well-defined dynamics~\cite{ames2014control,lindemann2018control,sadigh2016safe}.  A central tenet of these approaches is to define safety in a precise and analytical way.  For instance, core notions of safety have been characterized by strict reachability conditions for analytically defined state spaces~\cite{bertsekas2012dynamic,mitchell2005time} or forward-invariance properties with respect to geometric safe sets~\cite{prajna2004safety,ames2016control}. 
 However, the conditions under which safety is studied have evolved with the growing integration of deep learning into robotics.  New challenges, such as the non-linearity and scale of neural network-based policies, have prompted efforts to learn constraints and filters directly from data, resulting in tools aimed at ensuring safety in more complex settings~\cite{robey2020learning,achiam2017constrained,cheng2019end}.  And yet, despite this progress, the recent merger of robotics with foundation models---characterized by their fusion of multiple data modalities and billions of tuned parameters~\cite{driess2023palm,brohan2023rt}---has both introduced new threat models and widened the gap between the scalability and applicability of existing approaches to robot safety.  


A core challenge in ensuring the safety of AI-enabled robots lies in the immense capabilities of foundation models, which significantly broaden the range of achievable robotic behaviors~\cite{black2024pi_0,o2023open}.  Unlike classical approaches, modern notions of safety are increasingly semantic and contextual, particularly as AI-enabled robots are equipped with the ability to understand natural language, respond to visual inputs, and interact with high-fidelity world models~\cite{zhou2024multimodal}.  For example, consider a humanoid tasked with pouring boiling water from a kettle. The safety of this task depends on the context. If a cup is under the spout, the action is safe; if a hand covers the cup, it is unsafe.  Such scenarios illustrate the need for new algorithms and benchmarks that evaluate the contextual safety of AI-enabled robots as well as their underlying foundation models.

As a standalone technology, foundation models such as large language models (LLMs) are trained through a process called \emph{alignment} to generate content that aligns with human values~\cite{ouyang2022training,rafailov2024direct}.  And while at face value the proliferation of alignment techniques employed when training foundation models has reduced the propagation of harmful content~\cite{bai2022constitutional,guan2024deliberative,dubey2024llama}, it has been continually demonstrated that malicious users can elicit harmful content from these models through a class of attacks known as jailbreaking~\cite{wei2024jailbroken,zou2023universal,chao2023jailbreaking}.  Jailbreaking attacks are generally designed to produce textual prompts that, when entered into a foundation model, produce toxic text (e.g., bomb-building instructions), images (e.g., depictions of violence), or video (e.g., pornography).  In response to the threat posed by jailbreaking attacks, research in the AI safety community has proposed mitigation strategies, including filters~\cite{inan2023llama,jain2023baseline}, defense algorithms~\cite{zou2024improving,robey2023smoothllm}, and evaluation protocols designed to detect malignant capabilities~\cite{chao2024jailbreakbench,carlsmith2023scheming,greenblatt2024stress}.  And while several classes of attacks remain effective against state-of-the-art models~\cite{russinovich2024great,li2024llm}, existing defenses have greatly reduced the susceptibility of LLMs to jailbreaking.

Although alignment algorithms and jailbreaking defenses are generally effective in steering general-purpose foundation models away from harmful outputs, this enhanced robustness does not extend to LLM-enabled robots.  Attacks on textual models tend to target the generation of harmful information (\textit{e.g.}, bomb-building instructions) rather than actions (\textit{e.g.}, detonating a bomb in the real world).  For this reason, existing defenses overlook robotic risks, which evolve over time, depend heavily on context, and, crucially, have immediate physical consequences in the real world.  In other words, ensuring the safety of an LLM is not sufficient to ensure the safety of an LLM-enabled robot.  And indeed, recent work has indicated that jailbreaking attacks on LLM-enabled robots are remarkably effective at eliciting harmful actions (\textit{e.g.}, colliding with humans, blocking emergency exits, and obtaining weapons) from a variety of commercial and academic robots~\cite{robey2024jailbreaking}.  This finding, alongside work that has discovered analogous AI vulnerabilities for web-based agents~\cite{wu2024adversarial}, cybersecurity systems~\cite{gupta2023chatgpt}, and search engines~\cite{nestaas2024adversarial}, indicates that general-purpose solutions are needed to mitigate malicious attacks in application-dependent settings, particularly given the distinct possibility of these attacks causing harm in the physical world.
