The field of robotics has been fundamentally transformed by robotic foundation models, which have enabled breakthroughs in applications such as manipulation~\cite{liang2023code,arenas2024prompt, chen2022nlmapsaycan, huang2022inner}, autonomous driving~\cite{li2024driving,fan2024learning, schumann-2023-velma, sharan2023llm}, service robotics~\cite{rana2023sayplan, llm_service_robot, momallm24}, robot-assisted surgery~\cite{kim2024surgical,schmidgall2024general}, and navigation~\cite{pmlr-v205-shah23b, pmlr-v229-shah23c, xie2023reasoning}. The capability sets of these models have grown so rapidly that numerous robotic systems controlled by foundation models---including the Unitree Go2, Agility Digit, and Figure 01---are now available directly to consumers and actively deployed in homes, warehouses, and offices~\cite{zeng2023large,wang2024large}.  Moreover, in the years ahead, current scaling trends and architectural advances (see, e.g.,~\cite{sartor2024neural,pearce2024scaling}) indicate that the next generation of AI-enabled robots will automate labor traditionally performed by humans~\cite{ahn2024autort,strobel2024llm2swarm,guarascio2024will}. It is therefore essential that the safety of any technology at the intersection of AI and robotics be rigorously scrutinized as these systems are increasingly deployed collaboratively alongside humans~\cite{kim2024understanding}.


Robot safety has traditionally been viewed through the lens of robust and adaptive control, which collectively aim to synthesize policies that account for worst-case disturbances, uncertain dynamics, and evolving environments~\cite{zhou1998essentials,aastrom1995adaptive,mayne2000constrained,bemporad2007robust}. Classical tools, including temporal logic and control barrier functions, are also widely used to certify safety given well-defined dynamics~\cite{ames2014control,lindemann2018control,sadigh2016safe}.  A central tenet of these approaches is to define safety in a precise and analytical way.  For instance, core notions of safety have been characterized by strict reachability conditions for analytically defined state spaces~\cite{bertsekas2012dynamic,mitchell2005time} or forward-invariance properties with respect to geometric safe sets~\cite{prajna2004safety,ames2016control}. 
 However, the conditions under which safety is studied have evolved with the growing integration of deep learning into robotics.  New challenges, such as the non-linearity and scale of neural network-based policies, have prompted efforts to learn constraints and filters directly from data, resulting in tools aimed at ensuring safety in more complex settings~\cite{robey2020learning,achiam2017constrained,cheng2019end}.  And yet, despite this progress, the recent merger of robotics with foundation models---characterized by their fusion of multiple data modalities and billions of tuned parameters~\cite{driess2023palm,brohan2023rt}---has both introduced new threat models and widened the gap between the scalability and applicability of existing approaches to robot safety.  


A core challenge in ensuring the safety of AI-enabled robots lies in the immense capabilities of foundation models, which significantly broaden the range of achievable robotic behaviors~\cite{black2024pi_0,o2023open}.  Unlike classical approaches, modern notions of safety are increasingly semantic and contextual, particularly as AI-enabled robots are equipped with the ability to understand natural language, respond to visual inputs, and interact with high-fidelity world models~\cite{zhou2024multimodal}.  For example, consider a humanoid tasked with pouring boiling water from a kettle. The safety of this task depends on the context. If a cup is under the spout, the action is safe; if a hand covers the cup, it is unsafe.  Such scenarios illustrate the need for new algorithms and benchmarks that evaluate the contextual safety of AI-enabled robots as well as their underlying foundation models.

As a standalone technology, foundation models such as large language models (LLMs) are trained through a process called \emph{alignment} to generate content that aligns with human values~\cite{ouyang2022training,rafailov2024direct}.  And while at face value the proliferation of alignment techniques employed when training foundation models has reduced the propagation of harmful content~\cite{bai2022constitutional,guan2024deliberative,dubey2024llama}, it has been continually demonstrated that malicious users can elicit harmful content from these models through a class of attacks known as jailbreaking~\cite{wei2024jailbroken,zou2023universal,chao2023jailbreaking}.  Jailbreaking attacks are generally designed to produce textual prompts that, when entered into a foundation model, produce toxic text (e.g., bomb-building instructions), images (e.g., depictions of violence), or video (e.g., pornography).  In response to the threat posed by jailbreaking attacks, research in the AI safety community has proposed mitigation strategies, including filters~\cite{inan2023llama,jain2023baseline}, defense algorithms~\cite{zou2024improving,robey2023smoothllm}, and evaluation protocols designed to detect malignant capabilities~\cite{chao2024jailbreakbench,carlsmith2023scheming,greenblatt2024stress}.  And while several classes of attacks remain effective against state-of-the-art models~\cite{russinovich2024great,li2024llm}, existing defenses have greatly reduced the susceptibility of LLMs to jailbreaking.

Although alignment algorithms and jailbreaking defenses are generally effective in steering general-purpose foundation models away from harmful outputs, this enhanced robustness does not extend to LLM-enabled robots.  Attacks on textual models tend to target the generation of harmful information (\textit{e.g.}, bomb-building instructions) rather than actions (\textit{e.g.}, detonating a bomb in the real world).  For this reason, existing defenses overlook robotic risks, which evolve over time, depend heavily on context, and, crucially, have immediate physical consequences in the real world.  In other words, ensuring the safety of an LLM is not sufficient to ensure the safety of an LLM-enabled robot.  And indeed, recent work has indicated that jailbreaking attacks on LLM-enabled robots are remarkably effective at eliciting harmful actions (\textit{e.g.}, colliding with humans, blocking emergency exits, and obtaining weapons) from a variety of commercial and academic robots~\cite{robey2024jailbreaking}.  This finding, alongside work that has discovered analogous AI vulnerabilities for web-based agents~\cite{wu2024adversarial}, cybersecurity systems~\cite{gupta2023chatgpt}, and search engines~\cite{nestaas2024adversarial}, indicates that general-purpose solutions are needed to mitigate malicious attacks in application-dependent settings, particularly given the distinct possibility of these attacks causing harm in the physical world.



This paper provides a general safety architecture, which we term \textsc{RoboGuard}, that addresses the unique safety challenges inherent to using LLMs in robotics.  We motivate this architecture by first proposing a desiderata for defenses against attacks on LLM-enabled robots, which we hope guides future research on this topic.  Our key contributions are as follows:
% We first propose a desiderata for such an architecture that may guide the development of candidate safety approaches. 
% We then introduce \textsc{RoboGuard}, a general safety architecture for LLM-enabled robots. 
Our key contributions are as follows:

\begin{enumerate}[left=0em]
    \item [1.] We propose the following desiderata to guide the development of broadly applicable safety solutions for LLM-enabled robots: \textit{contextual attack mitigation}, \textit{utility}, \textit{efficiency}, and \textit{applicability}. Together, these desiderata outline the challenges of safeguarding LLM-enabled robots across a range of environments and LLM planning approaches, while allowing for nominal robot behavior.
    \item [2.] We develop \textsc{RoboGuard}, a two-stage guardrail architecture for ensuring the safety of LLM-enabled robots. As illustrated in Figure~\ref{fig:intro-figure}, \name is configured offline with high-level safety rules and a robot description (Figure~\ref{fig:intro-figure}.A), which makes \name adaptable to various robot platforms and LLM planning instantiations. 
    Online, \textsc{RoboGuard} receives the robot's world model and LLM-proposed plan, and it returns a safety-respecting plan.
    This is achieved with the following two innovations.
    \item [3.] \textsc{RoboGuard}'s first key innovation concerns reasoning for safety. 
    \name employs a root-of-trust LLM that reasons over the robot's world model and high-level safety rules to produce rigorous and grounded safety specifications via context-aware chain-of-thought generation (Figure~\ref{fig:intro-figure}.B).
    By decoupling potentially malicious prompts from pre-defined safety rules, the root-of-trust LLM is robust to adversarial or otherwise unsafe prompts.
    \item [4.] \textsc{RoboGuard}'s second key innovation resolves potential conflicts between the inferred safety specifications and the, potentially malicious, user LLM-generated plan.
    \textsc{RoboGuard} allows for arbitrary LLM-planning APIs, 
    so we first translate the LLM-generated plan into a contextually grounded temporal logic specification.
    We then employ tools from controller synthesis to generate  a robot plan that maximally follows user preferences while ensuring that safety specifications are satisfied (Figure~\ref{fig:intro-figure}.C).
\end{enumerate}


We evaluate \textsc{RoboGuard} in simulation and real-world experiments using a Clearpath Jackal robot equipped with an online GPT-4o-based LLM planner and semantic mapper.
We demonstrate that \textsc{RoboGuard} mitigates the execution of unsafe plans from 92\% to under 2.5\% without compromising performance on safe plans.
Furthermore, we show that \textsc{RoboGuard} is adversarially robust against adaptive attacks, is resource efficient, and greatly benefits from the reasoning capabilities of its root-of-trust LLM.


% Via simulation and real-world experiments using WHICH ROBOT?, we show that \name successfully mitigates attacks while allowing for safe behavior, is adversarially robust, and resource efficient.(EXPAND? FROM ABSTRACT?)

In the rest of the paper, we discuss related work in Section~\ref{sec:related-work}
We introduce some key technical aspects of our method in Section~\ref{sec:preliminaries}.
We present our guardrail in Section~\ref{sec:method} and evaluate it in Section~\ref{sec:experiments}.
Finally, we discuss our guardrail's limitations in Section~\ref{sec:limitations} and conclude in Section~\ref{sec:conclusion}.

